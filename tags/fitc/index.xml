<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>FITC on matsueushi</title>
    <link>https://matsueushi.github.io/tags/fitc/</link>
    <description>Recent content in FITC on matsueushi</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 04 Jul 2019 10:10:28 -0400</lastBuildDate>
    
	<atom:link href="https://matsueushi.github.io/tags/fitc/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ガウス過程の変数補助法をJuliaで実装、回帰結果を比較</title>
      <link>https://matsueushi.github.io/posts/ivm/</link>
      <pubDate>Thu, 04 Jul 2019 10:10:28 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/ivm/</guid>
      <description>前回書いた 「ガウス過程の補助変数法 (Inducing variable method) を理解する 」の続き。 Julia (v1.0) を使って、前回調べた SoD, SoR, DTC, FITC による回帰の近似結果を実際に確認する。
簡単のため、gp.jl により、
 ガウスカーネルのクラス GaussianKernel が定義されていて、
 カーネル k に対してカーネル関数 \( k(x, y) \) と相互共分散 \( (k(x_i, y_j))_{i,j}\) がそれぞれ ker(k, x, y) と cov(k, xs, ys) で計算できる  と仮定する。ということで、まずはライブラリの読み込み。
using Distributions using Plots using LinearAlgebra include(&amp;#34;gp.jl&amp;#34;) データは、MLPシリーズ 「ガウス過程と機械学習」の pp.157, 図5.3 補助入力点の配置 と同じサンプルを使う。上記のページに掲載されている、「補助変数法の例 (1次元の場合).」のデータの生成方法を参考にして次のようにデータを100個作成した。
# サンプルデータの作成 xs = vcat(rand(80),rand(20) * 3 .+ 1.0) sort!(xs) fx = sin.</description>
    </item>
    
    <item>
      <title>ガウス過程の補助変数法 (Inducing variable method) を理解する</title>
      <link>https://matsueushi.github.io/posts/sparse-approximate-gp/</link>
      <pubDate>Thu, 27 Jun 2019 00:55:38 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/sparse-approximate-gp/</guid>
      <description>6&amp;frasl;27 追記: typo, \( p(\mathbf{y} | \mathbf{f}) \) の誤字を修正, \( q_{\text{FITC}}(\mathbf{f}_* | \mathbf{y}) \) の二番目の等号を修正 (\( \sigma^{-2} \) を削除)
「ガウス過程と機械学習」を読んでいるが、5.2補助変数法のところで、どの部分で近似が行われているのかよく分からなくなってしまった。
そのため、今回は原論文であるQuinonero Candela, J. and Rasmussen, CE.の &amp;ldquo;A Unifying View of Sparse Approximate Gaussian Process Regression&amp;rdquo; を読んでスパース近似についてまとめて見ようと思う。ゴールは、The Fully Independent Training Conditional (FITC) の理解である。
\( \mathbf{X}=(\mathbf{x}_1, \ldots, \mathbf{x}_N) \) を学習データ、 \( \mathbf{y}=(y_1, \ldots, y_N)^\top \) を観測値とする。学習データと観測値の関係は、ガウス過程から生成される関数 \( f \) と誤差 \( \epsilon_n \) を用いて
$$ y_n = f(\mathbf{x_n}) + \epsilon_n,$$ $$\epsilon_n \sim \mathcal{N}(0, \sigma^2)$$</description>
    </item>
    
  </channel>
</rss>