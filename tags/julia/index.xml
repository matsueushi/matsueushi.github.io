<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Julia on matsueushi</title>
    <link>https://matsueushi.github.io/tags/julia/</link>
    <description>Recent content in Julia on matsueushi</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 08 Jun 2019 20:08:12 -0400</lastBuildDate>
    
	<atom:link href="https://matsueushi.github.io/tags/julia/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Juliaでガウス過程を実装&amp;パラメーター推定</title>
      <link>https://matsueushi.github.io/posts/gp-parameter-estimation/</link>
      <pubDate>Sat, 08 Jun 2019 20:08:12 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/gp-parameter-estimation/</guid>
      <description>「ガウス過程と機械学習」を3章まで読み終えたので、復習を兼ねてJulia(1.1.0)でガウス過程を実装し、 カーネルのハイパーパラメーターをOptim.jlで推定するところまでをまとめる。数学的に細かい内容は本を読んで欲しい。 図3.23の陸上男子100mの世界記録の回帰モデルを作成することを今回の目標とする。
ガウスカーネルによる回帰: ガウスカーネル＋線形カーネルによる回帰: 任意の有限の入力 \( x_1, \ldots , x_n \) を与えたときに、出力 \( (f(x_1), \ldots , f(x_n)) \) が平均 \( (\mu(x_1), \ldots , \mu(x_n)) \) 分散 \( (k(x_n, x_{nm} )) \) のガウス分布に従う時、 \( f \) をガウス過程と呼び、 \( f \sim \text{GP} (\mu(x), k(x, x^\prime)) \) と書く。そして \( \mu \) を平均関数、 \( k \) をカーネル関数と呼んでいるのであった。
今回は本と同様、簡単のために平均関数が恒等的に0となるものだけを考える。
ガウスカーネルの定義 もっとも基本的なカーネルであるガウスカーネルを定義して、ガウス過程を構成する。ガウスカーネルのカーネル関数は次のものとする。 $$ k(x, x^\prime ) = \exp \left( -\frac{|x-x^\prime|^2}{\theta}\right) $$ 本文では $$ k(x, x^\prime ) = \theta_1 \exp \left( -\frac{|x-x^\prime|^2}{\theta_2}\right) $$ この形で紹介されていたが、後々カーネルの線型結合を考えるのでここでは \( exp \) の前に係数を付けない前者を採用する。</description>
    </item>
    
    <item>
      <title>ガウス過程と機械学習: 3.5まで</title>
      <link>https://matsueushi.github.io/posts/gp-nlp-2/</link>
      <pubDate>Sun, 19 May 2019 22:37:52 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/gp-nlp-2/</guid>
      <description>引き続き「ガウス過程と機械学習(第二刷)」を読み進めJuliaで実装している。
ハイパーパラメーターの最適化(勾配を使わず、Optim.jlの optimize を使ってしまった)のところまで読み進めた。
 3.4.2のガウス過程回帰の計算を行う際、予測分布の分散共分散行列が計算誤差の影響で対称行列にならずエラーが発生することがあったので、場合によっては対称化が必要。 図3.16のガウスカーネル
\( \begin{aligned} k(x, x^\prime) = \theta_1 \exp \left( - \frac{|x-x^\prime|^2}{\theta_2} \right) \end{aligned} \) のパラメーター推定で、\( (\theta_1, \theta_2, \theta_3)=(1, 0.4, 0.1) \) とすると下のようになり本と違ってしまった。  \( (\theta_1, \theta_2, \theta_3)=(1, 0.4, 0.01) \) とすると近い図になる(全く同じには見えない)
 尤度の計算が合わなかった。尤度を図示した図3.16で-5未満を切り捨てるとうまくいかなかった。20以下を切り捨てると近い図になった。  本文の局所解(ii)に該当する点の尤度は-2.0299となり本文の-1.934とは違ってしまった。
図3.20のパラメーター推定は正しくできたが、こちらも対数尤度が違ってしまった((a):本文-1.788、実装-1.738, (b):本文-2.174, 実装-2.5029) 詳細は下のレポジトリ、ノートブックを見て下さい。更新は下のMedium用のブランチではなく、masterの方に行う予定です。
https://github.com/matsueushi/gp_and_mlp/tree/blog-2019-05-19
https://nbviewer.jupyter.org/github/matsueushi/gp_and_mlp/blob/blog-2019-05-19/gp.ipynb</description>
    </item>
    
    <item>
      <title>「ガウス過程と機械学習 」を読み始めた</title>
      <link>https://matsueushi.github.io/posts/gp-nlp-1/</link>
      <pubDate>Fri, 10 May 2019 22:30:45 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/gp-nlp-1/</guid>
      <description>持橋・大羽の「ガウス過程と機会学習」を読み始めた。Juliaでコードを書きながら内容を確かめている。
本を読むまで定義も理解していなかったレベルだったが、無限次元のガウス分布を考えるというモチベーションから「有限次元に制限すれば(通常の)ガウス分布になるもの」としてガウス過程の定義が出てくるのは非常に自然だと思った。
分散共分散行列の成分を作る時に使われるカーネル \( k(x,x^\prime) \) は \( x \) と \(x^\prime \) の「近さ」を表す関数とでも考えれば良いのだろうか。
なんでそういうことを考えるのかという気持ちの部分が丁寧に説明されているので意図がわからずに数式の中に闇雲に迷い込むことなく今の所楽しく読み進められている。
エラッタ:
http://chasen.org/~daiti-m/gpbook/errata.html
https://scrapbox.io/GPandML2019/support
3.3の「ガウス過程とカーネル」のところまで読んだ。
自分が躓いた点
 “3.2.4 ガウス過程からのサンプル”で図3.9のようなサンプルを実装するときは正則化項を入れないと計算がうまくいかないことがある(1.4 リッジ回帰の部分で触れられている)。著者のサンプルコードでは非常に正則化項として1e-6を導入していた。共分散行列の計算の際に対角成分に正規化項を加えればよい。 “3.3.1 ガウス過程のRBFカーネル”で、線形モデルの基底関数のグリッドを無限に細かくするとRBFカーネルになると書かれている部分は、本文中の基底関数を使うと \( H \rightarrow \infty \) とした時にカーネル関数がRBF関数に収束しない。基底関数に \( 1 / \sqrt{H} \) を掛けたものを考えればOK “3.3.2 さまざまなカーネル”で線形カーネルを実装するときに、カーネル関数は dot(x1, x2) ではなく、必ず1となる入力の最初の次元も考慮して 1 + dot(x1, x2) とする。他のカーネルの場合は x1 — x2 の計算の段階で消えるので考慮する必要はない また、Matérnカーネルを定義する際に、Juliaでは第二種のベッセル関数は SpecialFunctions の besselk を使えば良い。ベッセル関数は \( x=0 \) で特異性を持つので、カーネル関数 k(x1, x2) を定義するときは x1 = x2 の時に条件分岐で1を返すようにすればいい  カーネルとガウシアン過程を定義したjlファイル:</description>
    </item>
    
    <item>
      <title>Distribution.jlで分布をシフト・スケールさせる</title>
      <link>https://matsueushi.github.io/posts/shift-scale-distribution/</link>
      <pubDate>Tue, 30 Apr 2019 21:58:27 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/shift-scale-distribution/</guid>
      <description>公式ドキュメントでは記述が見つけられなかったが、 LocationScale を使えば良いっぽい。
 Distributions.jl/src/univariate/continuous/locationscale.jl https://github.com/JuliaStats/Distributions.jl/blob/master/src/univariate/continuous/locationscale.jl
 例えば、PyMC3の StudentT
pm.StudentT(&amp;#39;x&amp;#39;, nu=nu, mu=mu, lam=lam) に対応する分布をDistribution.jlで使いたいとき、Distribution.jlでの TDist のパラメトライズは nu だけで mu や lam は指定できないが、
LocationScale(mu, lam, TDist(nu)) とすればOK。PyMCの StudentT に出ている pdf と同じグラフを書いて確認しよう。
using Distributions using Plots using Printf # StudentT分布のシフト x = -8:0.01:8 mu = [0, 0, -2, -2] lam = [1, 1, 1, 2] nu = [1, 5, 5, 5] plt = Plots.plot() for i in 1:4 y = pdf.(LocationScale(mu[i], lam[i], TDist(nu[i])), x) Plots.</description>
    </item>
    
    <item>
      <title>Juliaで体験するベイズ推論(7) - The Price Is Right</title>
      <link>https://matsueushi.github.io/posts/bayesian-methods-julia-7/</link>
      <pubDate>Wed, 17 Apr 2019 21:44:20 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/bayesian-methods-julia-7/</guid>
      <description>引き続き「Pythonで体験するベイズ推論」のJulia+Mambaによる実装に挑戦している。わざわざ特別Mediumに書くような題材は無いな、と思っていたのだが、第5章の「例題 : テレビ番組 “The Price Is Right”の最適化」のモデリング( pm.potential が出てくるところ)でちょっと詰まったので、Mambaでの実装について記しておく。
問題を単純化すると、
 二つの賞品A, Bの合計価格(これを真の価格と今後呼ぶ)を予想したい 真の価格は正規分布 \( \text{Normal}(35000, 7500^2) \) に従うと仮定する 賞品A, Bの価格の事前分布はそれぞれ正規分布 \( \text{Normal}(12000, 3000^2) \) , \( \text{Normal}(3000,500^2) \) に従うと仮定する このような条件のモデリングである。  実際にモデリングをやってみて、賞品A, Bの事前分布と、その和をモデリングするところまでは下のようにすればいいので簡単であるのだが、( using 等は略した)真の価格の分布とサンプリングした賞品A, Bの価格の分布の和を結びつける段階で、はて？？？となった。
data = Dict{Symbol, Any}( :data_mu =&amp;gt; [3e3, 12e3], :data_std =&amp;gt; [5e2, 3e3], :mu_prior =&amp;gt; 35e3, :std_prior =&amp;gt; 75e2, ) model = Model( prize = Stochastic(1, (data_mu, data_std) -&amp;gt; MvNormal(data_mu, data_std)), price_estimate = Logical(prize -&amp;gt; sum(prize)), ) モデルを下のようにしてしまうと、</description>
    </item>
    
    <item>
      <title>Juliaで体験するベイズ推論(6) -スペースシャトル「チャレンジャー号」の悲劇</title>
      <link>https://matsueushi.github.io/posts/bayesian-methods-julia-6/</link>
      <pubDate>Wed, 10 Apr 2019 00:24:53 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/bayesian-methods-julia-6/</guid>
      <description>最近はGaussianRandomWalkを使った時系列ベイズモデルの推定に挑戦していたが、あまりうまくいかなかったので一旦「Pythonで体験するベイズ推論」に戻ろうと思う。
今回は「Pythonで体験するベイズ推論」の「2.2.27 例題 カンニングした学生の割合」をJuliaで実装した内容を紹介する。
まずはライブラリのインポート
using Distributed addprocs(3) using CSV using DataFrames using HTTP using LaTeXStrings using LinearAlgebra @everywhere using Mamba using Plots データの加工はこのような形で行った。DataFrame で Int64 にパースしたい行にMissing valueやNaNがあるとき、convertではエラーになるので、 パースできない場合は missing になる tryparse を使って、その後 nothing になる行を削除して、Union{Nothing, Int64} から Int64 にもう一度変換している。
r = HTTP.request(&amp;#34;GET&amp;#34;, &amp;#34;https://git.io/vXknD&amp;#34;); challengers_data = CSV.read(IOBuffer(r.body)) names!(challengers_data, [:date, :temperature, :incident]) # incidentのパース challengers_data[:incident] = tryparse.(Int64, challengers_data[:incident]) # NaNを削除 challengers_data = challengers_data[challengers_data[:incident] .!= nothing, :] challengers_data[:incident] = convert.(Int64, challengers_data[:incident]) disallowmissing!(challengers_data) データの図示をする。weighted_color_mean を使って、マーカーの色を青から赤にグラデーションさせた。</description>
    </item>
    
    <item>
      <title>JuliaでRollingWindow</title>
      <link>https://matsueushi.github.io/posts/rollingwindow/</link>
      <pubDate>Sun, 07 Apr 2019 00:29:01 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/rollingwindow/</guid>
      <description>Pandasの pd.rolling のようなことをJuliaで行うためのメモ
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html
 複雑なことがやりたければこれを使えば良さそうだ。
 JeffreySarnoff/RollingFunctions.jl
https://github.com/JeffreySarnoff/RollingFunctions.jl
 </description>
    </item>
    
    <item>
      <title>Juliaの2次元のArrayを1次元にする / Juliaの3次元のArrayを2次元にする</title>
      <link>https://matsueushi.github.io/posts/julia-array-dim/</link>
      <pubDate>Fri, 05 Apr 2019 00:20:33 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/julia-array-dim/</guid>
      <description>やり方がよくわからなくなるので自分用メモを兼ねて。JuliaのVersion は1.1.0 (2019–01–21)。
2次元 julia&amp;gt; x = [1 2; 3 4] 2×2 Array{Int64,2}: 1 2 3 4 縦方向に拾っていきたいときは簡単で、 vec
julia&amp;gt; vec(x) 4-element Array{Int64,1}: 1 3 2 4 横方向に拾いたい場合は permutedims を噛ませる。
julia&amp;gt; vec(permutedims(x)) 4-element Array{Int64,1}: 1 2 3 4 &amp;nbsp;transpose と collect を使う
julia&amp;gt; vec(collect(transpose(x))) 4-element Array{Int64,1}: 1 2 3 4 だと場合によっては問題があり、 Transpose は LinearAlgebra の操作なので、例えば Array{Char, 2} に対しては定義されていないので実行できない。
julia&amp;gt; [&amp;#39;a&amp;#39; &amp;#39;b&amp;#39;;&amp;#39;c&amp;#39; &amp;#39;d&amp;#39;] 2×2 Array{Char,2}: &amp;#39;a&amp;#39; &amp;#39;b&amp;#39; &amp;#39;c&amp;#39; &amp;#39;d&amp;#39; julia&amp;gt; transpose([&amp;#39;a&amp;#39; &amp;#39;b&amp;#39;;&amp;#39;c&amp;#39; &amp;#39;d&amp;#39;]) 2×2 LinearAlgebra.</description>
    </item>
    
    <item>
      <title>Mamba.jlでGaussianRandomWalkを作って使う</title>
      <link>https://matsueushi.github.io/posts/mamba-gaussianranwomwalk/</link>
      <pubDate>Wed, 03 Apr 2019 21:52:38 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/mamba-gaussianranwomwalk/</guid>
      <description>PyMC3にはTimeseriesとして GaussianRandomWalk などの時系列モデルが実装されている。
https://docs.pymc.io/api/distributions/timeseries.html#pymc3.distributions.timeseries.GaussianRandomWalk
だが残念なことに私が使っているMamba.jl(0.12.1)には時系列モデルがない。下のように cumsum を使ってモデルを作成することは可能ではあるが、面倒だし次元が大きくなってきたりモデルが複雑になってくると遅い。
local_level_model = Model( obs = Stochastic(1, (N, T, sigma_I) -&amp;gt; MvNormal(T, sigma_I), false ), T = Logical(1, (T_0, disturbance) -&amp;gt; T_0 .+ vcat([0], cumsum(disturbance)), ), disturbance = Stochastic(1, (N, sigma_T) -&amp;gt; MvNormal(N - 1, sigma_T), false ), sigma_I = Stochastic(() -&amp;gt; InverseGamma()), sigma_T = Stochastic(() -&amp;gt; InverseGamma()), T_0 = Stochastic(T_init -&amp;gt; Normal(T_init, 100)), ) だからと言ってそのためにわざわざPython+PyMC3に移るのも癪だし、練習を兼ねてJulia+Mamba用の確率分布を試しに作ってみようと思ったのが今回の内容である。幸いなことに、Mamba.jlには作り方のガイドラインが書いてあるので、多変量分布用のガイドラインを参考にして作ることができた。
今回は、GaussianRandomWalk は、初期値の分布 \( D \) とドリフト \( \mu_i \) , 分散 \( \sigma \) とした時に、</description>
    </item>
    
    <item>
      <title>Mamba.jl v0.12.0のStackOverflowError:</title>
      <link>https://matsueushi.github.io/posts/mamba-v-0-12-0/</link>
      <pubDate>Sun, 31 Mar 2019 21:47:30 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/mamba-v-0-12-0/</guid>
      <description>GCPに環境を作っていて気づいたのだが、Distibution.jlのv0.17.0と関数が干渉していて using Mamba すると下のようなワーニングが出て、
WARNING: Method definition (::Type{Distributions.DiscreteNonParametric{Int64, P, Base.OneTo{Int64}, Ps} where Ps where P})(AbstractArray{T&amp;lt;:Real, 1}) where {T&amp;lt;:Real} in module Distributions at /Users/apple/.julia/packages/Distributions/fMt8c/src/univariate/discrete/categorical.jl:40 overwritten in module Mamba at /Users/apple/.julia/packages/Mamba/qNBKz/src/distributions/constructors.jl:7. WARNING: Method definition (::Type{Distributions.MvNormal{T, Cov, Mean} where Mean&amp;lt;:(AbstractArray{T, 1} where T) where Cov&amp;lt;:(PDMats.AbstractPDMat{T} where T&amp;lt;:Real) where T&amp;lt;:Real})(AbstractArray{T&amp;lt;:Real, 1}, Real) where {T&amp;lt;:Real} in module Distributions at /Users/apple/.julia/packages/Distributions/fMt8c/src/multivariate/mvnormal.jl:200 overwritten in module Mamba at /Users/apple/.julia/packages/Mamba/qNBKz/src/distributions/constructors.jl:35. WARNING: Method definition (::Type{Distributions.MvNormalCanon{T, P, V} where V&amp;lt;:(AbstractArray{T, 1} where T) where P&amp;lt;:(PDMats.</description>
    </item>
    
    <item>
      <title>GCPでJuliaのノートブックを実行</title>
      <link>https://matsueushi.github.io/posts/gcp-julia/</link>
      <pubDate>Sat, 30 Mar 2019 21:37:57 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/gcp-julia/</guid>
      <description>自分用メモ
ローカルの公開鍵をGCPのインスタンスに登録
 [秘密鍵/公開鍵]GCPにSSHで接続する方法
https://sleepless-se.net/2018/09/15/gcp-ssh/
 Juliaのバイナリをダウンロード
curl -OL https://julialang-s3.julialang.org/bin/linux/x64/1.1/julia-1.1.0-linux-x86_64.tar.gz 解凍
$ sudo mkdir /bin/julia $ sudo tar xvzf julia-1.1.0-linux-x86_64.tar.gz -C /bin/julia $ /bin/julia/julia-1.1.0/bin/julia _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type &amp;#34;?&amp;#34; for help, &amp;#34;]?&amp;#34; for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.</description>
    </item>
    
    <item>
      <title>Plots.jlでx軸、y軸のスケールを揃える</title>
      <link>https://matsueushi.github.io/posts/plotly-scale/</link>
      <pubDate>Sun, 24 Mar 2019 21:43:34 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/plotly-scale/</guid>
      <description>&amp;nbsp;aspece_ratio=1 とすれば良いだけだった。簡単だった。
using Distributions using Plots x = y = 0:0.5:5 exp_x = Exponential(3) exp_y = Exponential(10) z = Surface((s, t) -&amp;gt; pdf(exp_x, s) * pdf(exp_y, t), x, y) heatmap(x, y, z) heatmap(x, y, z, aspect_ratio = 1) </description>
    </item>
    
    <item>
      <title>Arrayを横に並べてMatrixにする</title>
      <link>https://matsueushi.github.io/posts/julia-array-matrix/</link>
      <pubDate>Sat, 23 Mar 2019 21:36:07 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/julia-array-matrix/</guid>
      <description>自分用メモ。Juliaで Array を横に並べて Matrix にする方法は、repeat で第1引数を1にして、第2引数を並べたい個数にすればいい。
julia&amp;gt; xs = [1, 2, 3, 4, 5] 5-element Array{Int64,1}: 1 2 3 4 5 julia&amp;gt; repeat([1, 2, 3, 4, 5], 1, 4) 5×4 Array{Int64,2}: 1 1 1 1 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5</description>
    </item>
    
    <item>
      <title>Juliaで体験するベイズ推論(5) -嘘に対抗するアルゴリズム</title>
      <link>https://matsueushi.github.io/posts/bayesian-methods-julia-5/</link>
      <pubDate>Fri, 22 Mar 2019 21:22:17 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/bayesian-methods-julia-5/</guid>
      <description>「Pythonで体験するベイズ推論」の「2.2.7 例題 カンニングした学生の割合」をやってみよう。
学生が試験中にカンニングする頻度を求めたい。観測データは個人がカンニングしたかどうかは特定できない、以下のアルゴリズムを用いる。
 コイントスを(こっそり)行い、表が出たら正直に答える 裏が出た場合、もう一枚コインを(こっそり)投げ、表が出たら「カンニングした」と答え、裏が出たら「カンニングしなかった」と答える。  カンニングが全くなければ、「カンニングした」という回答がある確率は 1&amp;frasl;2 * 1&amp;frasl;2 = 1&amp;frasl;4, 全員カンニングしていれば 1&amp;frasl;2 + 1&amp;frasl;2 * 1&amp;frasl;2 = 3&amp;frasl;4 になる。  まずはライブラリをインポート。
using Distributed addprocs(3) @everywhere using Mamba using Plots 学生の数が100人、「カンニングした」という回答が35人とする。
N = 100 X = 35 Mambaのモデルは次のようになる。
model = Model( obs = Stochastic( (proportion, N) -&amp;gt; Binomial(N, proportion), false ), proportion = Logical( (true_answers, first_coin_flips, second_coin_flips) -&amp;gt; (observed = @.(first_coin_flips * true_answers + (1 - first_coin_flips) * second_coin_flips); mean(observed)), false ), true_answers = Stochastic(1, p -&amp;gt; Bernoulli(p), false), first_coin_flips = Stochastic(1, () -&amp;gt; Bernoulli(0.</description>
    </item>
    
    <item>
      <title>Juliaで体験するベイズ推論(4) -ベイズ的 A/B</title>
      <link>https://matsueushi.github.io/posts/bayesian-methods-julia-4/</link>
      <pubDate>Thu, 21 Mar 2019 21:05:35 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/bayesian-methods-julia-4/</guid>
      <description>今回はMambaを使って、「Pythonで体験するベイズ推論」の「例題 : ベイズ的 A/B」 をモデリングする。
例題 : ベイズ的 A/B テスト A/Bテストの例題を解いてみよう。
サイトAを見せられたユーザーが最終的にコンバージョンにつながる確率を \( p_A \)と仮定し、\( N \) 人がサイトAを見せられて、そのうち \( n \) 人がコンバージョンにつながったとする。
まずはベルヌーイ分布を使って、\( N \) 回の試行をシミュレートする。
# 定数をセット p_true = 0.05 N = 1500 occurrences = rand(Bernoulli(p_true), N) Mamba.jlで推論アルゴリズムを作成すると、次のようになる。\( p \) の事前分布は \( [0, 1] \) の一様分布に従うとしている。
model0 = Model( obs = Stochastic(1, (p, N) -&amp;gt; UnivariateDistribution[Bernoulli(p) for _ in 1:N], false), p = Stochastic(() -&amp;gt; Uniform()), ) モデルは、等価な次の形で書いた方が単純になってわかりやすいかもしれない。</description>
    </item>
    
    <item>
      <title>Juliaで体験するベイズ推論(3) -新しいデータセットの生成</title>
      <link>https://matsueushi.github.io/posts/bayesian-methods-julia-3/</link>
      <pubDate>Wed, 20 Mar 2019 20:58:08 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/bayesian-methods-julia-3/</guid>
      <description>引き続き「Pythonで体験するベイズ推論」の第2章の新しいデータセットの作成をJuliaでやってみる。
新しいデータセットの生成 PyMCについての説明はスキップして、シミュレーションによるメッセージ数のデータ生成から行う。
Mamba.jlは分布の作成にDistributions.jlを使っているので、シミュレーションだけ行いたかったらDistributionsを using すれば十分。
using Distributions using Plots データセットの作成とプロットは下のようになる。
function plot_artificial_sms_dataset() tau = rand(DiscreteUniform(0, 80)) theta = 20 lambda_1, lambda_2 = rand(Exponential(theta), 2) lambda_ = cat(fill(lambda_1, tau), fill(lambda_2, 80 - tau), dims = 1) data = @.rand(Poisson(lambda_)) barc = fill(1, 80) barc[tau] = 2 bar(0:80-1, data, linecolor = :transparent, fillcolor = barc, xlabel = &amp;#34;Time (days)&amp;#34;, ylabel = &amp;#34;Count of messages&amp;#34;, label = &amp;#34;&amp;#34;) end plts = [] for i in 1:4 push!</description>
    </item>
    
    <item>
      <title>Juliaで体験するベイズ推論(2) -メッセージ数に変化はあるか？ </title>
      <link>https://matsueushi.github.io/posts/bayesian-methods-julia-2/</link>
      <pubDate>Wed, 20 Mar 2019 20:45:02 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/bayesian-methods-julia-2/</guid>
      <description>前回に引き続き、「Pythonで体験するベイズ推論」をJuliaでやってみる。本に従い、前回作成した「メッセージ数に変化はあるか？」を二つの変化点の場合に拡張する。
変化点が二つの場合を考えてみる。モデルは変化点が一つの場合とほぼ同じで、
model2 = Model( obs = Stochastic(1, (lambda, N) -&amp;gt; UnivariateDistribution[Poisson(lambda[i]) for i in 1:N], false ), lambda = Logical(1, (lambda1, lambda2, lambda3, tau1, tau2, N) -&amp;gt; (out = fill(lambda1, N); i1 = Int64(tau1.value) + 1; # Juliaは1-indexingのため i2 = Int64(tau2.value) + 1; out[i1:end] .= lambda2; out[i2:end] .= lambda3; out), false, ), lambda1 = Stochastic(theta -&amp;gt; Exponential(theta)), lambda2 = Stochastic(theta -&amp;gt; Exponential(theta)), lambda3 = Stochastic(theta -&amp;gt; Exponential(theta)), tau1 = Stochastic(N -&amp;gt; DiscreteUniform(0, N-1)), tau2 = Stochastic((tau1, N) -&amp;gt; DiscreteUniform(tau1, N)), ) 初期値とサンプリングスキームを同様に与えてサンプリングすると、</description>
    </item>
    
    <item>
      <title>JuliaでArray of Arrayを1次元Vectorにする方法</title>
      <link>https://matsueushi.github.io/posts/julia-array-of-array/</link>
      <pubDate>Wed, 20 Mar 2019 00:33:19 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/julia-array-of-array/</guid>
      <description>よく忘れるのでメモ。
結論から言うと Base.Iterators.flatten を適用して collect すれば良い。
julia&amp;gt; import Base.Iterators: flatten julia&amp;gt; xs = [[1, 2, 3], [4, 5, 6]] 2-element Array{Array{Int64,1},1}: [1, 2, 3] [4, 5, 6] julia&amp;gt; collect(flatten(xs)) 6-element Array{Int64,1}: 1 2 3 4 5 6 追記(2019/4/7):
&amp;nbsp;vcat を使った方が簡潔だった。
julia&amp;gt; vcat(xs...) 6-element Array{Int64,1}: 1 2 3 4 5 6</description>
    </item>
    
    <item>
      <title>Juliaで体験するベイズ推論(1) -メッセージ数に変化はあるか？</title>
      <link>https://matsueushi.github.io/posts/bayesian-methods-julia-1/</link>
      <pubDate>Tue, 19 Mar 2019 23:30:41 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/bayesian-methods-julia-1/</guid>
      <description>久保拓弥「データ解析のための統計モデリング入門」を読み終えたので、次はCameron Davidson-Pilon著、玉木徹訳の「Pythonで体験するベイズ推論」(GitHubリポジトリ) をJuliaとMamba.jlでモデリングしていきたいと思う。
まず例題1.4.1の「メッセージ数に変化はあるか？」をやってみる。
元のノートブックはこれ。
JuliaでCSVファイルをhttps上から取るには、パッケージ HTTP , CSV をインポートして
r = HTTP.request(&amp;#34;GET&amp;#34;, &amp;#34;https://git.io/vXTVC&amp;#34;); count_data = CSV.read(IOBuffer(r.body), header=[&amp;#34;messages&amp;#34;]) とすれば良い。 Plotsでプロットすると下のような感じ。Juliaはインデックスが1から始まるので、x軸を 0:N-1 にしておく。
N = length(count_data.messages) bar(0:N-1, count_data.messages, label = &amp;#34;&amp;#34;, size = [600, 200], linecolor = :transparent, xlabel = &amp;#34;Time (days)&amp;#34;, ylabel = &amp;#34;Count of messages&amp;#34;) \( i \) 日目のメッセージ数 \( C_i \) がポアソン分布 \( \text{Poisson}(\lambda) \) に従い、\( \lambda \) の値が突然ある日 \( \lambda \) で変わる日があるとする。\( \tau &amp;lt; \lambda \) のとき \( \lambda = \lambda_1, \tau \ge \lambda \) の時 \( \lambda=\lambda_2 \)とする。</description>
    </item>
    
    <item>
      <title>Julia版「データ解析のための統計モデリング入門」読書ノート</title>
      <link>https://matsueushi.github.io/posts/julia-kubo/</link>
      <pubDate>Mon, 04 Mar 2019 23:01:57 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/julia-kubo/</guid>
      <description>最近、久保拓弥「データ解析のための統計モデリング入門――一般化線形モデル・階層ベイズモデル・MCMC (確率と情報の科学)を読んでいる。
 生態学のデータ解析 - 久保拓弥
http://hosho.ees.hokudai.ac.jp/~kubo/ce/KuboTakuya.html
 本ではR + WinBUGSを使っているが、今回はJulia(1.1.0) + Mambaを使って実装した(10章まで)。実装はGithubに載せてある。
実装の中身は上のJupyter Notebookを見てもらうとして、それ以外に実装していて何点か躓いたことがあったので備忘のために記載しておく。
Binomial分布のGLM &amp;nbsp; formula で指定する説明変数は、0から1の間になっている必要がある。個体の数で割って、wtsに個体の数を指定してフィッティングを行う。
df.yy = df.y ./ df.N df.N = convert(Array{Float64}, df.N) result = glm(@formula(yy ~ x + f), df, Binomial(), wts = df.N) モデルのグラフ表示 Mambaのチュートリアルではモデルのグラフ表示にGraphViz.jlパッケージを使っている。
ただGraphViz.jlはメンテナンスが止まっているようで、Julia v0.7以降では動かないようである。
ForneyLab.jlがGraphViz.jlの後継としてモデルのグラフ表示をサポートしているので、こちらを使用すれば良い。
Multiprocessing Mambaは addproc でプロセスを追加して、
using Distributed addprocs(3) Mambaをインポートするときに @everywhere を付ければMambaが自動的にMCMCのチェインごとのサンプリングを並列化してくれる。(section10.ipynb参照)
@everywhere using Mamba 11章の空間構造のある階層ベイズモデル(intrinsic Gaussian CARモデル)も実装できたらアップデートしたい。
3/14追記: 11章の空間構造のある階層ベイズモデルも実装したのでアップデートした。これで一通り読破したことになるのかな？次は以前読もうとして諦めた「Pythonで体験するベイズ推論」をJuliaでやってみようか。</description>
    </item>
    
  </channel>
</rss>