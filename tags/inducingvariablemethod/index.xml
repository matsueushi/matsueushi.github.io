<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>InducingVariableMethod on matsueushi</title>
    <link>https://matsueushi.github.io/tags/inducingvariablemethod/</link>
    <description>Recent content in InducingVariableMethod on matsueushi</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 27 Jun 2019 00:55:38 -0400</lastBuildDate>
    
	<atom:link href="https://matsueushi.github.io/tags/inducingvariablemethod/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ガウス過程の補助変数法 (Inducing variable method) を理解する</title>
      <link>https://matsueushi.github.io/posts/sparse-approximate-gp/</link>
      <pubDate>Thu, 27 Jun 2019 00:55:38 -0400</pubDate>
      
      <guid>https://matsueushi.github.io/posts/sparse-approximate-gp/</guid>
      <description>「ガウス過程と機械学習」を読んでいるが、5.2補助変数法のところで、どの部分で近似が行われているのかよく分からなくなってしまった。
そのため、今回は原論文であるQuinonero Candela, J. and Rasmussen, CE.の &amp;ldquo;A Unifying View of Sparse Approximate Gaussian Process Regression&amp;rdquo; を読んでスパース近似についてまとめて見ようと思う。ゴールは、The Fully Independent Training Conditional (FITC) の理解である。
\( \mathbf{X}=(\mathbf{x}_1, \ldots, \mathbf{x}_N) \) を学習データ、 \( \mathbf{y}=(y_1, \ldots, y_N)^\top \) を観測値とする。学習データと観測値の関係は、ガウス過程から生成される関数 \( f \) と誤差 \( \epsilon_n \) を用いて
$$ y_n = f(\mathbf{x_n}) + \epsilon_n,$$ $$\epsilon_n \sim \mathcal{N}(0, \sigma^2)$$
と結びつく観測モデルを考える。\( \mathbf{f}=(f_1, \ldots, f_N)^\top, f_n=f(\mathbf{x}_n) \) は学習データの出力値である。
取り組みたい問題は、ガウス過程回帰に基づいた回帰モデル。
予測したい点を \( \mathbf{X}_*=(\mathbf{x}_{*1}, \ldots, \mathbf{x}_{*M} ) \) , 出力値を \( \mathbf{f}_* \) , 観測値を \( \mathbf{y}_* \) とする。 上の観測モデルは、 $$ p(\mathbf{f} | \mathbf{y}) = \mathcal{N}(\mathbf{f}, \sigma^2\mathbf{I}) $$ と書き直せる。 \( \mathbf{f}, \mathbf{f}_* \) の条件付き同時確率分布はベイズルールから</description>
    </item>
    
  </channel>
</rss>